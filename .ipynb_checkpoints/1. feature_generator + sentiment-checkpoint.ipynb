{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- import lib and check version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 1.13.1\n",
      "Keras Version: 2.2.4-tf\n",
      "\n",
      "Python 3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 15:01:53) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "Pandas 1.0.1\n",
      "Scikit-Learn 0.22.1\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "# What version of Python do you have?\n",
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import demoji\n",
    "import pycld2 as cld2\n",
    "import datetime\n",
    "from textblob import TextBlob\n",
    "from textblob.taggers import NLTKTagger\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- init demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 1.14 seconds)\n",
      "\u001b[33mWriting emoji data to /Users/frostace/.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- start server service in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source activate bert_env\n",
    "# bert-serving-start -model_dir ./model/tmp/english_L-12_H-768_A-12/ -num_worker=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- start client service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "# bc.encode(['First do it', 'then do it right', 'then do it better'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load acronym dict and emoticon dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sub() missing 1 required positional argument: 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-469e4fcd947b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macronym_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mslang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macronym_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mslang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[.^$*+?|#]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0macronym_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslang\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macronym_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sub() missing 1 required positional argument: 'string'"
     ]
    }
   ],
   "source": [
    "acronym_table = pd.read_csv(\"./dictionary/acronym_dict.csv\")\n",
    "acronym_dict = dict()\n",
    "emoticon_table = pd.read_csv(\"./dictionary/emoticon_dict.csv\")\n",
    "emoticon_dict = dict()\n",
    "\n",
    "for row in range(acronym_table.shape[0]):\n",
    "    slang = acronym_table.iloc[row, 0]\n",
    "    slang = re.sub(r'[.^$*+?|#]', slang)\n",
    "    acronym_dict[slang] = acronym_table.iloc[row, 1]\n",
    "    \n",
    "for row in range(emoticon_table.shape[0]):\n",
    "    emoticon_dict[emoticon_table.iloc[row, 0]] = emoticon_table.iloc[row, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load exclamation dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'aha, ahem, ahh, ahoy, alas, arg, aw, bam, bingo, blah, boo, bravo, brrr, cheers, congratulations, dang, drat, darn, duh, eek, eh, encore, eureka, fiddlesticks, gadzooks, gee, gee, whiz, golly, goodbye, goodness, good grief, gosh, ha-ha, hallelujah, hello, hey, hmm, holy buckets, holy cow, holy smokes, hot dog, huh, humph, hurray, oh, oh dear, oh my, oh well, oops, ouch, ow, phew, phooey, pooh, rats, shh, shoo, thanks, there, tut-tut, uh-huh, uh-oh, ugh, wahoo, well, whoa, whoops, wow, yeah, yes, yikes, yippee, yo, yuck'\n",
    "exclamation_dict = set(s.split(', '))\n",
    "# generate exclamation pattern with the dictionary\n",
    "exclamation_pattern = r''\n",
    "for word in exclamation_dict:\n",
    "    exclamation_pattern += word + '|'\n",
    "exclamation_pattern = exclamation_pattern[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'wow']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(exclamation_pattern, \"OH my god wow\".lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = pd.read_csv(\"./data_tweets/tweets-10000-\" + \"2019-02-06\" +\".csv\", lineterminator='\\n')\n",
    "raw_tweet = csv_file.tweet.to_list()\n",
    "df = pd.DataFrame(columns = ['date', \n",
    "                             'tweet', \n",
    "                             'sentiment_score',\n",
    "                             'num_hashtags',\n",
    "                             'num_targets',\n",
    "                             'num_urls',\n",
    "                             'num_negations',\n",
    "                             'num_newlines',\n",
    "                             'num_pos_exclam',\n",
    "                             'num_neg_exclam',\n",
    "                             'num_pos_cap',\n",
    "                             'num_neg_cap',\n",
    "                             'num_pos_words',\n",
    "                             'num_neg_words',\n",
    "                             'num_JJ',\n",
    "                             'num_RB',\n",
    "                             'num_VB',\n",
    "                             'num_NN',\n",
    "                             'num_pos_JJ',\n",
    "                             'num_pos_RB',\n",
    "                             'num_pos_VB',\n",
    "                             'num_pos_NN',\n",
    "                             'num_neg_JJ',\n",
    "                             'num_neg_RB',\n",
    "                             'num_neg_VB',\n",
    "                             'num_neg_NN',\n",
    "                             'total_score_JJ',\n",
    "                             'total_score_RB',\n",
    "                             'total_score_VB',\n",
    "                             'total_score_NN',\n",
    "                             ])\n",
    "df['tweet'] = raw_tweet\n",
    "df['date'] = csv_file.date.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.25 s, sys: 35 ms, total: 5.28 s\n",
      "Wall time: 5.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# new\n",
    "csv_file = pd.read_csv(\"./data_tweets/tweets-10000-\" + \"2019-02-06\" +\".csv\", lineterminator='\\n')\n",
    "raw_tweet = csv_file.tweet.to_list()\n",
    "raw_date = csv_file.date.to_list()\n",
    "\n",
    "rows_to_drop = []\n",
    "# init lists to maintain new features\n",
    "'''# sentiment\n",
    "sentiment_score = []\n",
    "emoticon_score = [] # not available until resolve regex issue\n",
    "\n",
    "# twitter\n",
    "num_hashtags = []\n",
    "num_targets = []\n",
    "num_urls = []\n",
    "num_negations = []\n",
    "num_newlines = []\n",
    "num_ext_pos_emo = [] # not available until resolve regex issue\n",
    "num_ext_neg_emo = [] # not available until resolve regex issue\n",
    "num_pos_emo = [] # not available until resolve regex issue\n",
    "num_neg_emo = [] # not available until resolve regex issue\n",
    "num_pos_exclam = []\n",
    "num_neg_exclam = []\n",
    "num_pos_cap = []\n",
    "num_neg_cap = []\n",
    "num_pos_words = []\n",
    "num_neg_words = []\n",
    "\n",
    "# POS\n",
    "num_JJ = []\n",
    "num_RB = []\n",
    "num_VB = []\n",
    "num_NN = []\n",
    "num_pos_JJ = []\n",
    "num_pos_RB = []\n",
    "num_pos_VB = []\n",
    "num_pos_NN = []\n",
    "num_neg_JJ = []\n",
    "num_neg_RB = []\n",
    "num_neg_VB = []\n",
    "num_neg_NN = []\n",
    "total_score_JJ = []\n",
    "total_score_RB = []\n",
    "total_score_VB = []\n",
    "total_score_NN = []'''\n",
    "\n",
    "useful_pos = set(['JJ', 'RB', 'VB', 'NN'])\n",
    "\n",
    "for i in range(len(raw_tweet)):\n",
    "    line = raw_tweet[i]\n",
    "\n",
    "    ### Preprocessing\n",
    "    # filter other languages\n",
    "    isReliable, textBytesFound, details = cld2.detect(line)\n",
    "    if details[0][1] != 'en':\n",
    "        rows_to_drop.append(i)\n",
    "        continue\n",
    "\n",
    "    '''# substitute acronym slangs\n",
    "    for slang in acronym_dict:\n",
    "        print(slang)\n",
    "        line = re.sub(r'' + slang, acronym_dict[slang], line)\n",
    "\n",
    "    # remove emoticons and maintain an emoticon sentiment score\n",
    "    emoti_score = 0\n",
    "    emoti_count = {-2:0, -1:0, 0:0, 1:0, 2:0}\n",
    "    for emoti in emoticon_dict:\n",
    "        if emoti in line:\n",
    "            line = re.sub(r'' + emoti, '', line)\n",
    "            emoti_score += emoticon_dict[emoti]\n",
    "            # 2.2. # of extremely-pos, extremely-neg, positive, negative emoticons\n",
    "            emoti_count[emoticon_dict[emoti]] = emoti_count.get(emoticon_dict[emoti], 0) + 1\n",
    "    '''\n",
    "\n",
    "    # substitute emoji        \n",
    "    # emoji_pattern = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    # line = emoji_pattern.sub(r'', line)\n",
    "    line = demoji.replace(line)\n",
    "\n",
    "    # substitute url with ||U||\n",
    "    line = re.sub(r'(http\\S+)|((...)*.com(...)*)', \"||U||\", line)\n",
    "\n",
    "    # substitute #hashtag with ||H||\n",
    "    line = re.sub(r'\\#[a-zA-Z]+', \"||H||\", line)\n",
    "\n",
    "    # substitute @target with ||T||\n",
    "    line = re.sub(r'@[a-zA-Z]+', \"||T||\", line)\n",
    "\n",
    "    # substitute negations with NOT\n",
    "    for negation in ['not', 'no', 'never', 'n\\'t', 'cannot']:\n",
    "        line = re.sub(r'' + negation, \" NOT \", line)\n",
    "\n",
    "    # remove redundant spacing\n",
    "    line = re.sub(r'' + ' ' + '{2,}', ' ', line)\n",
    "\n",
    "    # substitute repeated sequence with 3 repeated characters\n",
    "    for ch in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
    "        line = re.sub(r'' + ch + '{3,}', ch + ch + ch, line)\n",
    "\n",
    "    # 1. sentiment features\n",
    "    line_blob = TextBlob(line)\n",
    "    sentiment_score = line_blob.sentiment.polarity\n",
    "    # emoticon_score.append(emoti_score)\n",
    "\n",
    "    # 2. twitter-specific features\n",
    "    # 2.1. # of hashtags, URLs, targets, negations, newlines\n",
    "    num_hashtags = line.count(\"||H||\")\n",
    "    num_targets = line.count(\"||T||\")\n",
    "    num_urls = line.count(\"||U||\")\n",
    "    num_negations = line.count(\"NOT\")\n",
    "    num_newlines = line.count(\"\\n\")\n",
    "\n",
    "    # 2.2. # of (+/-) capitalized words, exclamation words\n",
    "    cap_word_list = re.findall(r'[A-Z][\\'A-Z]+|[A-Z][a-zA-Z]+', line)\n",
    "    pos_cap_words_count = 0\n",
    "    neg_cap_words_count = 0\n",
    "    for word in cap_word_list:\n",
    "        word_polar = TextBlob(word).sentiment.polarity\n",
    "        if word_polar > 0:\n",
    "            pos_cap_words_count += 1\n",
    "        elif word_polar < 0:\n",
    "            neg_cap_words_count += 1\n",
    "\n",
    "    exclam_word_list = re.findall(exclamation_pattern, line.lower())\n",
    "    pos_exclam_words_count = 0\n",
    "    neg_exclam_words_count = 0\n",
    "    for word in exclam_word_list:\n",
    "        word_polar = TextBlob(word).sentiment.polarity\n",
    "        if word_polar > 0:\n",
    "            pos_exclam_words_count += 1\n",
    "        elif word_polar < 0:\n",
    "            neg_exclam_words_count += 1\n",
    "    num_pos_exclam = pos_exclam_words_count\n",
    "    num_neg_exclam = neg_exclam_words_count\n",
    "    num_pos_cap = pos_cap_words_count\n",
    "    num_neg_cap = neg_cap_words_count\n",
    "\n",
    "    # 2.3. # of positive words and negative words\n",
    "    pos_words_count = 0\n",
    "    neg_words_count = 0\n",
    "    wordlist = re.sub(\"[^\\w]\", \" \",  line).split() # substitute all non word characters with space\n",
    "    for word in wordlist:\n",
    "        word_polar = TextBlob(word).sentiment.polarity\n",
    "        if word_polar > 0:\n",
    "            pos_words_count += 1\n",
    "        elif word_polar < 0:\n",
    "            neg_words_count += 1\n",
    "    num_pos_words = pos_words_count\n",
    "    num_neg_words = neg_words_count\n",
    "\n",
    "    # 3. POS features\n",
    "    pos_count = {'JJ':0, 'RB':0, 'VB':0, 'NN':0}\n",
    "    pos_score = {'JJ':0, 'RB':0, 'VB':0, 'NN':0}\n",
    "    pos_polar_count = {'JJ+':0, 'RB+':0, 'VB+':0, 'NN+':0, 'JJ-':0, 'RB-':0, 'VB-':0, 'NN-':0}\n",
    "\n",
    "    pos_tuples = line_blob.tags\n",
    "    for word, pos_tag in pos_tuples:\n",
    "        if pos_tag in useful_pos:\n",
    "            word_polar = TextBlob(word).sentiment.polarity\n",
    "            if word_polar > 0:\n",
    "                appendix = '+'\n",
    "            elif word_polar < 0:\n",
    "                appendix = '-'\n",
    "            else:\n",
    "                appendix = ''\n",
    "\n",
    "            # 3.1. # of JJ, RB, VB, NN\n",
    "            pos_count[pos_tag] = pos_count.get(pos_tag, 0) + 1\n",
    "            # 3.2. ‚àë prior polarity scores of words for JJ, RB, VB, NN\n",
    "            pos_score[pos_tag] = pos_score.get(pos_tag, 0) + word_polar\n",
    "            # 3.3. # of (+/-) POS (JJ, RB, VB, NN)\n",
    "            pos_polar_count[pos_tag + appendix] = pos_polar_count.get(pos_tag + appendix, 0) + 1\n",
    "    num_JJ = pos_count['JJ']\n",
    "    num_RB = pos_count['RB']\n",
    "    num_VB = pos_count['VB']\n",
    "    num_NN = pos_count['NN']\n",
    "    num_pos_JJ = pos_polar_count['JJ+']\n",
    "    num_pos_RB = pos_polar_count['RB+']\n",
    "    num_pos_VB = pos_polar_count['VB+']\n",
    "    num_pos_NN = pos_polar_count['NN+']\n",
    "    num_neg_JJ = pos_polar_count['JJ-']\n",
    "    num_neg_RB = pos_polar_count['RB-']\n",
    "    num_neg_VB = pos_polar_count['VB-']\n",
    "    num_neg_NN = pos_polar_count['NN-']\n",
    "    total_score_JJ = pos_score['JJ']\n",
    "    total_score_RB = pos_score['RB']\n",
    "    total_score_VB = pos_score['VB']\n",
    "    total_score_NN = pos_score['NN']\n",
    "    df = pd.DataFrame(columns = ['date', \n",
    "                                 'tweet', \n",
    "                                 'sentiment_score',\n",
    "                                 'num_hashtags',\n",
    "                                 'num_targets',\n",
    "                                 'num_urls',\n",
    "                                 'num_negations',\n",
    "                                 'num_newlines',\n",
    "                                 'num_pos_exclam',\n",
    "                                 'num_neg_exclam',\n",
    "                                 'num_pos_cap',\n",
    "                                 'num_neg_cap',\n",
    "                                 'num_pos_words',\n",
    "                                 'num_neg_words',\n",
    "                                 'num_JJ',\n",
    "                                 'num_RB',\n",
    "                                 'num_VB',\n",
    "                                 'num_NN',\n",
    "                                 'num_pos_JJ',\n",
    "                                 'num_pos_RB',\n",
    "                                 'num_pos_VB',\n",
    "                                 'num_pos_NN',\n",
    "                                 'num_neg_JJ',\n",
    "                                 'num_neg_RB',\n",
    "                                 'num_neg_VB',\n",
    "                                 'num_neg_NN',\n",
    "                                 'total_score_JJ',\n",
    "                                 'total_score_RB',\n",
    "                                 'total_score_VB',\n",
    "                                 'total_score_NN',\n",
    "                                 ])\n",
    "\n",
    "    new_row = pd.DataFrame([[\n",
    "                            raw_date[i],\n",
    "                            raw_tweet[i],\n",
    "                            sentiment_score,\n",
    "                            num_hashtags,\n",
    "                            num_targets,\n",
    "                            num_urls,\n",
    "                            num_negations,\n",
    "                            num_newlines,\n",
    "                            num_pos_exclam,\n",
    "                            num_neg_exclam,\n",
    "                            num_pos_cap,\n",
    "                            num_neg_cap,\n",
    "                            num_pos_words,\n",
    "                            num_neg_words,\n",
    "                            num_JJ,\n",
    "                            num_RB,\n",
    "                            num_VB,\n",
    "                            num_NN,\n",
    "                            num_pos_JJ,\n",
    "                            num_pos_RB,\n",
    "                            num_pos_VB,\n",
    "                            num_pos_NN,\n",
    "                            num_neg_JJ,\n",
    "                            num_neg_RB,\n",
    "                            num_neg_VB,\n",
    "                            num_neg_NN,\n",
    "                            total_score_JJ,\n",
    "                            total_score_RB,\n",
    "                            total_score_VB,\n",
    "                            total_score_NN,\n",
    "                            ]], columns=df.columns)\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "# return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: Googloogl trading at 23 forward P/E; $AMZN = 3-4 P/S, FB = 22 forward P/E  Interest rates are near zero!  What is stretched?L)   https://rius.ca/social_story/201909080514444/ #Google #Maps #Vice #tech pic.twitter.com/nGOUqdWkHe83%aa%e3%83%bc%e3%82%b5%e3%83%ad%e3%83%b3%e3%80%90%e3%81%b2%e3%82%8d%e3%82%81%e3%81%a7%e3%81%83-%e3%81%82/ $NQ_F $CL_F $ZB_F $TLT $MET $PNC $CTL $LMT $WMT $TGT $DLTR $AMZN pic.twitter.com/EYl5a7CDIz"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_targets</th>\n",
       "      <th>num_urls</th>\n",
       "      <th>num_negations</th>\n",
       "      <th>num_newlines</th>\n",
       "      <th>num_pos_exclam</th>\n",
       "      <th>num_neg_exclam</th>\n",
       "      <th>...</th>\n",
       "      <th>num_pos_VB</th>\n",
       "      <th>num_pos_NN</th>\n",
       "      <th>num_neg_JJ</th>\n",
       "      <th>num_neg_RB</th>\n",
       "      <th>num_neg_VB</th>\n",
       "      <th>num_neg_NN</th>\n",
       "      <th>total_score_JJ</th>\n",
       "      <th>total_score_RB</th>\n",
       "      <th>total_score_VB</th>\n",
       "      <th>total_score_NN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-09-08 19:59:02</td>\n",
       "      <td>googlÁøªË®≥„Çµ„Ç§„Éà  https://translate.google.co.jp/tra...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-09-08 19:49:46</td>\n",
       "      <td>[VIDEO] These Key Stocks Are Looking For A Big...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-09-08 19:30:05</td>\n",
       "      <td>ACTIVE TRADERS Try one of these FREE trading g...</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-09-08 19:25:03</td>\n",
       "      <td>With past performance like this, how can you n...</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-09-08 19:00:18</td>\n",
       "      <td>Don't miss our next FREE OPTION TRADE.  Sign u...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2019-09-08 01:44:20</td>\n",
       "      <td>Alphabet's GOOG vs. GOOGL: What's the Differen...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2019-09-08 01:44:05</td>\n",
       "      <td>Google Cloud has a new program to assign its b...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2019-09-08 01:24:21</td>\n",
       "      <td>üîçGoogle's Chrome Has My Dead Grandpa's Data an...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2019-09-08 01:14:04</td>\n",
       "      <td>Google Maps is still pointing people who searc...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2019-09-08 00:46:12</td>\n",
       "      <td>$GOOGL is trading at 23 forward P/E; $AMZN = 3...</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows √ó 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date                                              tweet  \\\n",
       "0    2019-09-08 19:59:02  googlÁøªË®≥„Çµ„Ç§„Éà  https://translate.google.co.jp/tra...   \n",
       "2    2019-09-08 19:49:46  [VIDEO] These Key Stocks Are Looking For A Big...   \n",
       "3    2019-09-08 19:30:05  ACTIVE TRADERS Try one of these FREE trading g...   \n",
       "4    2019-09-08 19:25:03  With past performance like this, how can you n...   \n",
       "5    2019-09-08 19:00:18  Don't miss our next FREE OPTION TRADE.  Sign u...   \n",
       "..                   ...                                                ...   \n",
       "126  2019-09-08 01:44:20  Alphabet's GOOG vs. GOOGL: What's the Differen...   \n",
       "127  2019-09-08 01:44:05  Google Cloud has a new program to assign its b...   \n",
       "128  2019-09-08 01:24:21  üîçGoogle's Chrome Has My Dead Grandpa's Data an...   \n",
       "131  2019-09-08 01:14:04  Google Maps is still pointing people who searc...   \n",
       "132  2019-09-08 00:46:12  $GOOGL is trading at 23 forward P/E; $AMZN = 3...   \n",
       "\n",
       "     sentiment  num_hashtags  num_targets  num_urls  num_negations  \\\n",
       "0     0.000000             0            0         1              0   \n",
       "2     0.000000             0            0         1              0   \n",
       "3     0.133333             0            0         1              0   \n",
       "4     0.216667             0            0         1              1   \n",
       "5     0.200000             0            0         1              1   \n",
       "..         ...           ...          ...       ...            ...   \n",
       "126   0.000000             0            0         1              0   \n",
       "127   0.000000             0            0         1              0   \n",
       "128   0.000000             0            0         1              0   \n",
       "131   0.000000             0            0         1              0   \n",
       "132   0.037500             0            0         0              0   \n",
       "\n",
       "     num_newlines  num_pos_exclam  num_neg_exclam  ...  num_pos_VB  \\\n",
       "0               0               0               0  ...           0   \n",
       "2               0               0               0  ...           0   \n",
       "3               0               0               0  ...           0   \n",
       "4               0               0               0  ...           0   \n",
       "5               0               0               0  ...           0   \n",
       "..            ...             ...             ...  ...         ...   \n",
       "126             0               0               0  ...           0   \n",
       "127             0               0               0  ...           0   \n",
       "128             0               0               0  ...           0   \n",
       "131             0               0               0  ...           0   \n",
       "132             0               0               0  ...           0   \n",
       "\n",
       "     num_pos_NN  num_neg_JJ  num_neg_RB  num_neg_VB  num_neg_NN  \\\n",
       "0             0           0           0           0           0   \n",
       "2             0           0           0           0           0   \n",
       "3             0           0           0           0           1   \n",
       "4             0           1           0           0           0   \n",
       "5             0           0           0           0           0   \n",
       "..          ...         ...         ...         ...         ...   \n",
       "126           0           0           0           0           0   \n",
       "127           0           0           0           0           0   \n",
       "128           0           0           0           0           0   \n",
       "131           0           0           0           0           0   \n",
       "132           0           0           0           0           0   \n",
       "\n",
       "     total_score_JJ  total_score_RB  total_score_VB  total_score_NN  \n",
       "0              0.00             0.0             0.0        0.000000  \n",
       "2              0.00             0.0             0.0        0.000000  \n",
       "3              0.00             0.0             0.0       -0.133333  \n",
       "4              0.15             0.0             0.0        0.000000  \n",
       "5              0.00             0.0             0.0        0.000000  \n",
       "..              ...             ...             ...             ...  \n",
       "126            0.00             0.0             0.0        0.000000  \n",
       "127            0.00             0.0             0.0        0.000000  \n",
       "128            0.00             0.0             0.0        0.000000  \n",
       "131            0.00             0.0             0.0        0.000000  \n",
       "132            0.00             0.0             0.0        0.000000  \n",
       "\n",
       "[97 rows x 30 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "csv_file = pd.read_csv(\"./data_tweets/tweets-10000-\" + \"2019-09-08\" +\".csv\", lineterminator='\\n')\n",
    "raw_tweet = csv_file.tweet.to_list()\n",
    "\n",
    "rows_to_drop = []\n",
    "# init lists to maintain new features\n",
    "# sentiment\n",
    "sentiment_score = []\n",
    "emoticon_score = [] # not available until resolve regex issue\n",
    "\n",
    "# twitter\n",
    "num_hashtags = []\n",
    "num_targets = []\n",
    "num_urls = []\n",
    "num_negations = []\n",
    "num_newlines = []\n",
    "num_ext_pos_emo = [] # not available until resolve regex issue\n",
    "num_ext_neg_emo = [] # not available until resolve regex issue\n",
    "num_pos_emo = [] # not available until resolve regex issue\n",
    "num_neg_emo = [] # not available until resolve regex issue\n",
    "num_pos_exclam = []\n",
    "num_neg_exclam = []\n",
    "num_pos_cap = []\n",
    "num_neg_cap = []\n",
    "num_pos_words = []\n",
    "num_neg_words = []\n",
    "\n",
    "# POS\n",
    "num_JJ = []\n",
    "num_RB = []\n",
    "num_VB = []\n",
    "num_NN = []\n",
    "num_pos_JJ = []\n",
    "num_pos_RB = []\n",
    "num_pos_VB = []\n",
    "num_pos_NN = []\n",
    "num_neg_JJ = []\n",
    "num_neg_RB = []\n",
    "num_neg_VB = []\n",
    "num_neg_NN = []\n",
    "total_score_JJ = []\n",
    "total_score_RB = []\n",
    "total_score_VB = []\n",
    "total_score_NN = []\n",
    "\n",
    "useful_pos = set(['JJ', 'RB', 'VB', 'NN'])\n",
    "# new_row = pd.DataFrame([[slang, trans, trans_list[i]]], columns=df.columns)\n",
    "# df = pd.concat([df, new_row], ignore_index=True)\n",
    "for i in range(len(raw_tweet)):\n",
    "    line = raw_tweet[i]\n",
    "    \n",
    "    # remove non utf-8 encoded characters\n",
    "    line = re.sub(r'[^\\x00-\\x7f]', r'', str(line))\n",
    "    \n",
    "    sys.stdout.write('\\r')\n",
    "    # the exact output you're looking for:\n",
    "    sys.stdout.write(\"Testing: %s\" % (line))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    ### Preprocessing\n",
    "    # filter other languages\n",
    "    isReliable, textBytesFound, details = cld2.detect(line)\n",
    "    if details[0][1] != 'en':\n",
    "        rows_to_drop.append(i)\n",
    "        continue\n",
    "\n",
    "    '''# substitute acronym slangs\n",
    "    for slang in acronym_dict:\n",
    "        print(slang)\n",
    "        line = re.sub(r'' + slang, acronym_dict[slang], line)\n",
    "\n",
    "    # remove emoticons and maintain an emoticon sentiment score\n",
    "    emoti_score = 0\n",
    "    emoti_count = {-2:0, -1:0, 0:0, 1:0, 2:0}\n",
    "    for emoti in emoticon_dict:\n",
    "        if emoti in line:\n",
    "            line = re.sub(r'' + emoti, '', line)\n",
    "            emoti_score += emoticon_dict[emoti]\n",
    "            # 2.2. # of extremely-pos, extremely-neg, positive, negative emoticons\n",
    "            emoti_count[emoticon_dict[emoti]] = emoti_count.get(emoticon_dict[emoti], 0) + 1\n",
    "    '''\n",
    "\n",
    "    # substitute emoji        \n",
    "    # emoji_pattern = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    # line = emoji_pattern.sub(r'', line)\n",
    "    line = demoji.replace(line)\n",
    "\n",
    "    # substitute url with ||U||\n",
    "    line = re.sub(r'(http\\S+)|((...)*.com(...)*)', \"||U||\", line)\n",
    "\n",
    "    # substitute #hashtag with ||H||\n",
    "    line = re.sub(r'\\#[a-zA-Z]+', \"||H||\", line)\n",
    "\n",
    "    # substitute @target with ||T||\n",
    "    line = re.sub(r'@[a-zA-Z]+', \"||T||\", line)\n",
    "\n",
    "    # substitute negations with NOT\n",
    "    for negation in ['not', 'no', 'never', 'n\\'t', 'cannot']:\n",
    "        line = re.sub(r'' + negation, \" NOT \", line)\n",
    "\n",
    "    # remove redundant spacing\n",
    "    line = re.sub(r'' + ' ' + '{2,}', ' ', line)\n",
    "\n",
    "    # substitute repeated sequence with 3 repeated characters\n",
    "    for ch in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
    "        line = re.sub(r'' + ch + '{3,}', ch + ch + ch, line)\n",
    "\n",
    "    # 1. sentiment features\n",
    "    line_blob = TextBlob(line)\n",
    "    sentiment_score.append(line_blob.sentiment.polarity)\n",
    "    # emoticon_score.append(emoti_score)\n",
    "\n",
    "    # 2. twitter-specific features\n",
    "    # 2.1. # of hashtags, URLs, targets, negations, newlines\n",
    "    num_hashtags.append(line.count(\"||H||\"))\n",
    "    num_targets.append(line.count(\"||T||\"))\n",
    "    num_urls.append(line.count(\"||U||\"))\n",
    "    num_negations.append(line.count(\"NOT\"))\n",
    "    num_newlines.append(line.count(\"\\n\"))\n",
    "\n",
    "    # 2.2. # of (+/-) capitalized words, exclamation words\n",
    "    cap_word_list = re.findall(r'[A-Z][\\'A-Z]+|[A-Z][a-zA-Z]+', line)\n",
    "    pos_cap_words_count = 0\n",
    "    neg_cap_words_count = 0\n",
    "    for word in cap_word_list:\n",
    "        word_polar = TextBlob(word).sentiment.polarity\n",
    "        if word_polar > 0:\n",
    "            pos_cap_words_count += 1\n",
    "        elif word_polar < 0:\n",
    "            neg_cap_words_count += 1\n",
    "\n",
    "    exclam_word_list = re.findall(exclamation_pattern, line.lower())\n",
    "    pos_exclam_words_count = 0\n",
    "    neg_exclam_words_count = 0\n",
    "    for word in exclam_word_list:\n",
    "        word_polar = TextBlob(word).sentiment.polarity\n",
    "        if word_polar > 0:\n",
    "            pos_exclam_words_count += 1\n",
    "        elif word_polar < 0:\n",
    "            neg_exclam_words_count += 1\n",
    "    num_pos_exclam.append(pos_exclam_words_count)\n",
    "    num_neg_exclam.append(neg_exclam_words_count)\n",
    "    num_pos_cap.append(pos_cap_words_count)\n",
    "    num_neg_cap.append(neg_cap_words_count)\n",
    "\n",
    "    # 2.3. # of positive words and negative words\n",
    "    pos_words_count = 0\n",
    "    neg_words_count = 0\n",
    "    wordlist = re.sub(\"[^\\w]\", \" \",  line).split() # substitute all non word characters with space\n",
    "    for word in wordlist:\n",
    "        word_polar = TextBlob(word).sentiment.polarity\n",
    "        if word_polar > 0:\n",
    "            pos_words_count += 1\n",
    "        elif word_polar < 0:\n",
    "            neg_words_count += 1\n",
    "    num_pos_words.append(pos_words_count)\n",
    "    num_neg_words.append(neg_words_count)\n",
    "\n",
    "    # 3. POS features\n",
    "    pos_count = {'JJ':0, 'RB':0, 'VB':0, 'NN':0}\n",
    "    pos_score = {'JJ':0, 'RB':0, 'VB':0, 'NN':0}\n",
    "    pos_polar_count = {'JJ+':0, 'RB+':0, 'VB+':0, 'NN+':0, 'JJ-':0, 'RB-':0, 'VB-':0, 'NN-':0}\n",
    "\n",
    "    pos_tuples = line_blob.tags\n",
    "    for word, pos_tag in pos_tuples:\n",
    "        if pos_tag in useful_pos:\n",
    "            word_polar = TextBlob(word).sentiment.polarity\n",
    "            if word_polar > 0:\n",
    "                appendix = '+'\n",
    "            elif word_polar < 0:\n",
    "                appendix = '-'\n",
    "            else:\n",
    "                appendix = ''\n",
    "\n",
    "            # 3.1. # of JJ, RB, VB, NN\n",
    "            pos_count[pos_tag] = pos_count.get(pos_tag, 0) + 1\n",
    "            # 3.2. ‚àë prior polarity scores of words for JJ, RB, VB, NN\n",
    "            pos_score[pos_tag] = pos_score.get(pos_tag, 0) + word_polar\n",
    "            # 3.3. # of (+/-) POS (JJ, RB, VB, NN)\n",
    "            pos_polar_count[pos_tag + appendix] = pos_polar_count.get(pos_tag + appendix, 0) + 1\n",
    "    num_JJ.append(pos_count['JJ'])\n",
    "    num_RB.append(pos_count['RB'])\n",
    "    num_VB.append(pos_count['VB'])\n",
    "    num_NN.append(pos_count['NN'])\n",
    "    num_pos_JJ.append(pos_polar_count['JJ+'])\n",
    "    num_pos_RB.append(pos_polar_count['RB+'])\n",
    "    num_pos_VB.append(pos_polar_count['VB+'])\n",
    "    num_pos_NN.append(pos_polar_count['NN+'])\n",
    "    num_neg_JJ.append(pos_polar_count['JJ-'])\n",
    "    num_neg_RB.append(pos_polar_count['RB-'])\n",
    "    num_neg_VB.append(pos_polar_count['VB-'])\n",
    "    num_neg_NN.append(pos_polar_count['NN-'])\n",
    "    total_score_JJ.append(pos_score['JJ'])\n",
    "    total_score_RB.append(pos_score['RB'])\n",
    "    total_score_VB.append(pos_score['VB'])\n",
    "    total_score_NN.append(pos_score['NN'])\n",
    "\n",
    "\n",
    "csv_file['tweet'] = raw_tweet\n",
    "csv_file = csv_file.drop(rows_to_drop, axis=0)\n",
    "\n",
    "# sentiment features\n",
    "csv_file['sentiment'] = sentiment_score\n",
    "# csv_file['emoti_score'] = emoticon_score\n",
    "\n",
    "# twitter-specific features\n",
    "csv_file['num_hashtags'] = num_hashtags\n",
    "csv_file['num_targets'] = num_targets\n",
    "csv_file['num_urls'] = num_urls\n",
    "csv_file['num_negations'] = num_negations\n",
    "csv_file['num_newlines'] = num_newlines\n",
    "csv_file['num_pos_exclam'] = num_pos_exclam\n",
    "csv_file['num_neg_exclam'] = num_neg_exclam\n",
    "csv_file['num_pos_cap'] = num_pos_cap\n",
    "csv_file['num_neg_cap'] = num_neg_cap\n",
    "csv_file['num_pos_words'] = num_pos_words\n",
    "csv_file['num_neg_words'] = num_neg_words\n",
    "\n",
    "# pos features\n",
    "csv_file['num_JJ'] = num_JJ\n",
    "csv_file['num_RB'] = num_RB\n",
    "csv_file['num_VB'] = num_VB\n",
    "csv_file['num_NN'] = num_NN\n",
    "csv_file['num_pos_JJ'] = num_pos_JJ\n",
    "csv_file['num_pos_RB'] = num_pos_RB\n",
    "csv_file['num_pos_VB'] = num_pos_VB\n",
    "csv_file['num_pos_NN'] = num_pos_NN\n",
    "csv_file['num_neg_JJ'] = num_neg_JJ\n",
    "csv_file['num_neg_RB'] = num_neg_RB\n",
    "csv_file['num_neg_VB'] = num_neg_VB\n",
    "csv_file['num_neg_NN'] = num_neg_NN\n",
    "csv_file['total_score_JJ'] = total_score_JJ\n",
    "csv_file['total_score_RB'] = total_score_RB\n",
    "csv_file['total_score_VB'] = total_score_VB\n",
    "csv_file['total_score_NN'] = total_score_NN\n",
    "csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_cleaning(date_str):\n",
    "    csv_file = pd.read_csv(\"./data_tweets/tweets-10000-\" + date_str +\".csv\", lineterminator='\\n')\n",
    "    raw_tweet = csv_file.tweet.to_list()\n",
    "\n",
    "    rows_to_drop = []\n",
    "    # init lists to maintain new features\n",
    "    # sentiment\n",
    "    sentiment_score = []\n",
    "    emoticon_score = []\n",
    "\n",
    "    # twitter\n",
    "    num_hashtags = []\n",
    "    num_targets = []\n",
    "    num_urls = []\n",
    "    num_negations = []\n",
    "    num_newlines = []\n",
    "    num_ext_pos_emo = []\n",
    "    num_ext_neg_emo = []\n",
    "    num_pos_emo = []\n",
    "    num_neg_emo = []\n",
    "    num_pos_exclam = []\n",
    "    num_neg_exclam = []\n",
    "    num_pos_cap = []\n",
    "    num_neg_cap = []\n",
    "    num_pos_words = []\n",
    "    num_neg_words = []\n",
    "\n",
    "    # POS\n",
    "    num_JJ = []\n",
    "    num_RB = []\n",
    "    num_VB = []\n",
    "    num_NN = []\n",
    "    num_pos_JJ = []\n",
    "    num_pos_RB = []\n",
    "    num_pos_VB = []\n",
    "    num_pos_NN = []\n",
    "    num_neg_JJ = []\n",
    "    num_neg_RB = []\n",
    "    num_neg_VB = []\n",
    "    num_neg_NN = []\n",
    "    total_score_JJ = []\n",
    "    total_score_RB = []\n",
    "    total_score_VB = []\n",
    "    total_score_NN = []\n",
    "\n",
    "    useful_pos = set(['JJ', 'RB', 'VB', 'NN'])\n",
    "\n",
    "    for i in range(len(raw_tweet)):\n",
    "        line = raw_tweet[i]\n",
    "\n",
    "        ### Preprocessing\n",
    "        # remove non utf-8 encoded characters\n",
    "        line = re.sub(r'[^\\x00-\\x7f]', r'', str(line))\n",
    "        \n",
    "        # filter other languages\n",
    "        isReliable, textBytesFound, details = cld2.detect(line)\n",
    "        if details[0][1] != 'en':\n",
    "            rows_to_drop.append(i)\n",
    "            continue\n",
    "\n",
    "        '''lang_blob = TextBlob(line)\n",
    "        if lang_blob.detect_language() != 'en':\n",
    "            rows_to_drop.append(i)\n",
    "            # sentiment_score.append(0)\n",
    "            emoticon_score.append(0)\n",
    "            continue'''\n",
    "\n",
    "        '''# substitute acronym slangs\n",
    "        for slang in acronym_dict:\n",
    "            print(slang)\n",
    "            line = re.sub(r'' + slang, acronym_dict[slang], line)\n",
    "\n",
    "        # remove emoticons and maintain an emoticon sentiment score\n",
    "        emoti_score = 0\n",
    "        emoti_count = {-2:0, -1:0, 0:0, 1:0, 2:0}\n",
    "        for emoti in emoticon_dict:\n",
    "            if emoti in line:\n",
    "                line = re.sub(r'' + emoti, '', line)\n",
    "                emoti_score += emoticon_dict[emoti]\n",
    "                # 2.2. # of extremely-pos, extremely-neg, positive, negative emoticons\n",
    "                emoti_count[emoticon_dict[emoti]] = emoti_count.get(emoticon_dict[emoti], 0) + 1\n",
    "        '''\n",
    "\n",
    "        # substitute emoji        \n",
    "        # emoji_pattern = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "        # line = emoji_pattern.sub(r'', line)\n",
    "        line = demoji.replace(line)\n",
    "\n",
    "        # substitute url with ||U||\n",
    "        line = re.sub(r'(http\\S+)|((...)*.com(...)*)', \"||U||\", line)\n",
    "\n",
    "        # substitute #hashtag with ||H||\n",
    "        line = re.sub(r'\\#[a-zA-Z]+', \"||H||\", line)\n",
    "\n",
    "        # substitute @target with ||T||\n",
    "        line = re.sub(r'@[a-zA-Z]+', \"||T||\", line)\n",
    "\n",
    "        # substitute negations with NOT\n",
    "        for negation in ['not', 'no', 'never', 'n\\'t', 'cannot']:\n",
    "            line = re.sub(r'' + negation, \" NOT \", line)\n",
    "\n",
    "        # remove redundant spacing\n",
    "        line = re.sub(r'' + ' ' + '{2,}', ' ', line)\n",
    "\n",
    "        # substitute repeated sequence with 3 repeated characters\n",
    "        for ch in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
    "            line = re.sub(r'' + ch + '{3,}', ch + ch + ch, line)\n",
    "\n",
    "        # 1. sentiment features\n",
    "        line_blob = TextBlob(line)\n",
    "        sentiment_score.append(line_blob.sentiment.polarity)\n",
    "        # emoticon_score.append(emoti_score)\n",
    "\n",
    "        # 2. twitter-specific features\n",
    "        # 2.1. # of hashtags, URLs, targets, negations, newlines\n",
    "        num_hashtags.append(line.count(\"||H||\"))\n",
    "        num_targets.append(line.count(\"||T||\"))\n",
    "        num_urls.append(line.count(\"||U||\"))\n",
    "        num_negations.append(line.count(\"NOT\"))\n",
    "        num_newlines.append(line.count(\"\\n\"))\n",
    "\n",
    "        # 2.2. # of (+/-) capitalized words, exclamation words\n",
    "        cap_word_list = re.findall(r'[A-Z][\\'A-Z]+|[A-Z][a-zA-Z]+', line)\n",
    "        pos_cap_words_count = 0\n",
    "        neg_cap_words_count = 0\n",
    "        for word in cap_word_list:\n",
    "            word_polar = TextBlob(word).sentiment.polarity\n",
    "            if word_polar > 0:\n",
    "                pos_cap_words_count += 1\n",
    "            elif word_polar < 0:\n",
    "                neg_cap_words_count += 1\n",
    "\n",
    "        exclam_word_list = re.findall(exclamation_pattern, line.lower())\n",
    "        pos_exclam_words_count = 0\n",
    "        neg_exclam_words_count = 0\n",
    "        for word in exclam_word_list:\n",
    "            word_polar = TextBlob(word).sentiment.polarity\n",
    "            if word_polar > 0:\n",
    "                pos_exclam_words_count += 1\n",
    "            elif word_polar < 0:\n",
    "                neg_exclam_words_count += 1\n",
    "        num_pos_exclam.append(pos_exclam_words_count)\n",
    "        num_neg_exclam.append(neg_exclam_words_count)\n",
    "        num_pos_cap.append(pos_cap_words_count)\n",
    "        num_neg_cap.append(neg_cap_words_count)\n",
    "\n",
    "        # 2.3. # of positive words and negative words\n",
    "        pos_words_count = 0\n",
    "        neg_words_count = 0\n",
    "        wordlist = re.sub(\"[^\\w]\", \" \",  line).split() # substitute all non word characters with space\n",
    "        for word in wordlist:\n",
    "            word_polar = TextBlob(word).sentiment.polarity\n",
    "            if word_polar > 0:\n",
    "                pos_words_count += 1\n",
    "            elif word_polar < 0:\n",
    "                neg_words_count += 1\n",
    "        num_pos_words.append(pos_words_count)\n",
    "        num_neg_words.append(neg_words_count)\n",
    "\n",
    "        # 3. POS features\n",
    "        pos_count = {'JJ':0, 'RB':0, 'VB':0, 'NN':0}\n",
    "        pos_score = {'JJ':0, 'RB':0, 'VB':0, 'NN':0}\n",
    "        pos_polar_count = {'JJ+':0, 'RB+':0, 'VB+':0, 'NN+':0, 'JJ-':0, 'RB-':0, 'VB-':0, 'NN-':0}\n",
    "\n",
    "        pos_tuples = line_blob.tags\n",
    "        for word, pos_tag in pos_tuples:\n",
    "            if pos_tag in useful_pos:\n",
    "                word_polar = TextBlob(word).sentiment.polarity\n",
    "                if word_polar > 0:\n",
    "                    appendix = '+'\n",
    "                elif word_polar < 0:\n",
    "                    appendix = '-'\n",
    "                else:\n",
    "                    appendix = ''\n",
    "\n",
    "                # 3.1. # of JJ, RB, VB, NN\n",
    "                pos_count[pos_tag] = pos_count.get(pos_tag, 0) + 1\n",
    "                # 3.2. ‚àë prior polarity scores of words for JJ, RB, VB, NN\n",
    "                pos_score[pos_tag] = pos_score.get(pos_tag, 0) + word_polar\n",
    "                # 3.3. # of (+/-) POS (JJ, RB, VB, NN)\n",
    "                pos_polar_count[pos_tag + appendix] = pos_polar_count.get(pos_tag + appendix, 0) + 1\n",
    "        num_JJ.append(pos_count['JJ'])\n",
    "        num_RB.append(pos_count['RB'])\n",
    "        num_VB.append(pos_count['VB'])\n",
    "        num_NN.append(pos_count['NN'])\n",
    "        num_pos_JJ.append(pos_polar_count['JJ+'])\n",
    "        num_pos_RB.append(pos_polar_count['RB+'])\n",
    "        num_pos_VB.append(pos_polar_count['VB+'])\n",
    "        num_pos_NN.append(pos_polar_count['NN+'])\n",
    "        num_neg_JJ.append(pos_polar_count['JJ-'])\n",
    "        num_neg_RB.append(pos_polar_count['RB-'])\n",
    "        num_neg_VB.append(pos_polar_count['VB-'])\n",
    "        num_neg_NN.append(pos_polar_count['NN-'])\n",
    "        total_score_JJ.append(pos_score['JJ'])\n",
    "        total_score_RB.append(pos_score['RB'])\n",
    "        total_score_VB.append(pos_score['VB'])\n",
    "        total_score_NN.append(pos_score['NN'])\n",
    "\n",
    "\n",
    "    csv_file['tweet'] = raw_tweet\n",
    "    csv_file = csv_file.drop(rows_to_drop, axis=0)\n",
    "\n",
    "    # sentiment features\n",
    "    csv_file['sentiment'] = sentiment_score\n",
    "    # csv_file['emoti_score'] = emoticon_score\n",
    "\n",
    "    # twitter-specific features\n",
    "    csv_file['num_hashtags'] = num_hashtags\n",
    "    csv_file['num_targets'] = num_targets\n",
    "    csv_file['num_urls'] = num_urls\n",
    "    csv_file['num_negations'] = num_negations\n",
    "    csv_file['num_newlines'] = num_newlines\n",
    "    csv_file['num_pos_exclam'] = num_pos_exclam\n",
    "    csv_file['num_neg_exclam'] = num_neg_exclam\n",
    "    csv_file['num_pos_cap'] = num_pos_cap\n",
    "    csv_file['num_neg_cap'] = num_neg_cap\n",
    "    csv_file['num_pos_words'] = num_pos_words\n",
    "    csv_file['num_neg_words'] = num_neg_words\n",
    "\n",
    "    # pos features\n",
    "    csv_file['num_JJ'] = num_JJ\n",
    "    csv_file['num_RB'] = num_RB\n",
    "    csv_file['num_VB'] = num_VB\n",
    "    csv_file['num_NN'] = num_NN\n",
    "    csv_file['num_pos_JJ'] = num_pos_JJ\n",
    "    csv_file['num_pos_RB'] = num_pos_RB\n",
    "    csv_file['num_pos_VB'] = num_pos_VB\n",
    "    csv_file['num_pos_NN'] = num_pos_NN\n",
    "    csv_file['num_neg_JJ'] = num_neg_JJ\n",
    "    csv_file['num_neg_RB'] = num_neg_RB\n",
    "    csv_file['num_neg_VB'] = num_neg_VB\n",
    "    csv_file['num_neg_NN'] = num_neg_NN\n",
    "    csv_file['total_score_JJ'] = total_score_JJ\n",
    "    csv_file['total_score_RB'] = total_score_RB\n",
    "    csv_file['total_score_VB'] = total_score_VB\n",
    "    csv_file['total_score_NN'] = total_score_NN\n",
    "    \n",
    "    return csv_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convert tweet to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 2019-09-08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frostace/.conda/envs/bert_env/lib/python3.6/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=25\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 2020-02-06"
     ]
    }
   ],
   "source": [
    "# Inclusive on both sides\n",
    "start_date = datetime.date(2019, 9, 8)\n",
    "end_date = datetime.date(2020, 2, 6)\n",
    "time_window = (end_date - start_date).days + 1\n",
    "\n",
    "for single_date in (start_date + datetime.timedelta(n) for n in range(time_window)): \n",
    "    this_date = single_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    sys.stdout.write('\\r')\n",
    "    # the exact output you're looking for:\n",
    "    sys.stdout.write(\"Testing: %s\" % (this_date))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # clean data\n",
    "    tweet_table = data_cleaning(this_date)\n",
    "    \n",
    "    # acquire tweet text list and sentiment score list\n",
    "    tweet_list = tweet_table['tweet'].to_list()\n",
    "    sentiment_score_list = tweet_table['sentiment'].to_list()\n",
    "    \n",
    "    # generate BERT vector\n",
    "    tweet_vector_matrix = bc.encode(tweet_list)\n",
    "    tweet_vector_df = pd.DataFrame(tweet_vector_matrix)\n",
    "    \n",
    "    # insert sentiment score column\n",
    "    tweet_vector_df['sentiment'] = sentiment_score_list\n",
    "    tweet_vector_df['num_hashtags'] = tweet_table['num_hashtags'].to_list()\n",
    "    tweet_vector_df['num_targets'] = tweet_table['num_targets'].to_list()\n",
    "    tweet_vector_df['num_urls'] = tweet_table['num_urls'].to_list()\n",
    "    tweet_vector_df['num_negations'] = tweet_table['num_negations'].to_list()\n",
    "    tweet_vector_df['num_newlines'] = tweet_table['num_newlines'].to_list()\n",
    "    tweet_vector_df['num_pos_exclam'] = tweet_table['num_pos_exclam'].to_list()\n",
    "    tweet_vector_df['num_neg_exclam'] = tweet_table['num_neg_exclam'].to_list()\n",
    "    tweet_vector_df['num_pos_cap'] = tweet_table['num_pos_cap'].to_list()\n",
    "    tweet_vector_df['num_neg_cap'] = tweet_table['num_neg_cap'].to_list()\n",
    "    tweet_vector_df['num_pos_words'] = tweet_table['num_pos_words'].to_list()\n",
    "    tweet_vector_df['num_neg_words'] = tweet_table['num_neg_words'].to_list()\n",
    "    tweet_vector_df['num_JJ'] = tweet_table['num_JJ'].to_list()\n",
    "    tweet_vector_df['num_RB'] = tweet_table['num_RB'].to_list()\n",
    "    tweet_vector_df['num_VB'] = tweet_table['num_VB'].to_list()\n",
    "    tweet_vector_df['num_NN'] = tweet_table['num_NN'].to_list()\n",
    "    tweet_vector_df['num_pos_JJ'] = tweet_table['num_pos_JJ'].to_list()\n",
    "    tweet_vector_df['num_pos_RB'] = tweet_table['num_pos_RB'].to_list()\n",
    "    tweet_vector_df['num_pos_VB'] = tweet_table['num_pos_VB'].to_list()\n",
    "    tweet_vector_df['num_pos_NN'] = tweet_table['num_pos_NN'].to_list()\n",
    "    tweet_vector_df['num_neg_JJ'] = tweet_table['num_neg_JJ'].to_list()\n",
    "    tweet_vector_df['num_neg_RB'] = tweet_table['num_neg_RB'].to_list()\n",
    "    tweet_vector_df['num_neg_VB'] = tweet_table['num_neg_VB'].to_list()\n",
    "    tweet_vector_df['num_neg_NN'] = tweet_table['num_neg_NN'].to_list()\n",
    "    tweet_vector_df['total_score_JJ'] = tweet_table['total_score_JJ'].to_list()\n",
    "    tweet_vector_df['total_score_RB'] = tweet_table['total_score_RB'].to_list()\n",
    "    tweet_vector_df['total_score_VB'] = tweet_table['total_score_VB'].to_list()\n",
    "    tweet_vector_df['total_score_NN'] = tweet_table['total_score_NN'].to_list()\n",
    "    \n",
    "    tweet_vector_df.to_csv(\"./data_vectors/vectors-10000-\" + this_date + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- see original sentiment distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'hist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a4cf330fc3eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvector_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data_vectors/vectors-100-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mthis_date\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msentiment_scores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvector_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msentiment_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'hist'"
     ]
    }
   ],
   "source": [
    "# Inclusive on both sides\n",
    "start_date = datetime.date(2019, 2, 6)\n",
    "end_date = datetime.date(2020, 2, 6)\n",
    "time_window = (end_date - start_date).days + 1\n",
    "\n",
    "sentiment_scores = []\n",
    "for single_date in (start_date + datetime.timedelta(n) for n in range(time_window)): \n",
    "    this_date = single_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    vector_table = pd.read_csv(\"./data_vectors/vectors-100-\" + this_date +\".csv\", lineterminator='\\n')\n",
    "    sentiment_scores += vector_table.sentiment.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   80.,   191.,   422.,   700.,  1280., 25635.,  1668.,  1145.,\n",
       "          208.,   275.]),\n",
       " array([-1. , -0.8, -0.6, -0.4, -0.2,  0. ,  0.2,  0.4,  0.6,  0.8,  1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_env_py3.6",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
